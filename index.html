<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Unified Multi-Modal Interactive and Reactive 3D Motion Generation via Rectified Flow. A Contrastive Rectified Flow based generative model using interactive RAG based on LLM decomposition for 3D interactive and reactive motion generation.">
  <meta property="og:title" content="Unified Multi-Modal Interactive and Reactive 3D Motion Generation via Rectified Flow"/>
  <meta property="og:description" content="We present the MDD dataset: 620 minutes of dance motion capture across 15 genres, 10K+ text annotations, enabling novel Text2Duet and Text2DanceAccompaniment tasks."/>
  <meta property="og:url" content="https://jaysmehta208.github.io/mdd/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Unified Multi-Modal Interactive and Reactive 3D Motion Generation via Rectified Flow">
  <meta name="twitter:description" content="A multimodal dataset for generating duet dances conditioned on text and music. A Contrastive Rectified Flow based generative model using interactive RAG based on LLM decomposition for 3D interactive and reactive motion generation.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>MDD</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Unified Multi-Modal Interactive and Reactive 3D Motion Generation via Rectified Flow</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://gprerit96.github.io" target="_blank">Prerit Gupta</a>,</span>
                <span class="author-block">
                  <a href="https://shouryaverma.github.io/" target="_blank">Shourya Verma</a>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.purdue.edu/homes/ayg/" target="_blank">Ananth Grama</a>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.purdue.edu/homes/ab/" target="_blank">Aniket Bera</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Purdue University, West Lafayette<br>ICLR 2026</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- ICCV PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/forum?id=QaAgHKbJop" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>OpenReview</span>
                      </a>
                    </span>

                     <!-- ArXiv abstract Link -->
                  <span class="link-block">
                    <a href="https://arxiv.org/abs/2509.24099" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/gprerit96/DualFlow" target="_blank"
                    class="external-link button is-normal is-rounded is-dark" disabled>
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (to be released)</span>
                  </a>
                </span>

                 
                    
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Static Images Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <!-- Image 1 -->
      <div class="item mb-5">
        <div class="has-text-centered">
          <img src="static/images/Dualflow_teaser.png"
               alt="DualFlow Teaser"
               style="max-width: 90%; height: auto; margin: 0 auto;" />
        </div>
        <h2 class="subtitle is-6 has-text-centered">
          Our DualFlow model unifies two tasks: (a) Interactive Motion Generation, which synthesizes synchronized two-person interactions, (b) Reactive Motion Generation, which generates responsive motions for Person B (red) conditioned on Person A’s (blue) movements. The generation process is conditioned jointly on text, music, and the retrieved motion samples.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End Static Images Section -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Generating realistic, context-aware two-person motion conditioned on diverse modalities remains a fundamental challenge for graphics, animation and embodied AI systems. Real-world applications such as VR/AR companions, social robotics and game agents require models capable of producing coordinated interpersonal behavior while flexibly switching between interactive and reactive generation. We introduce DualFlow, the first unified and efficient framework for multi-modal two-person motion generation. DualFlow conditions 3D motion generation on diverse inputs, including text, music, and prior motion sequences. Leveraging rectified flow, it achieves deterministic straight-line sampling paths between noise and data, reducing inference time and mitigating error accumulation common in diffusion-based models. To enhance semantic grounding, DualFlow employs a novel Retrieval-Augmented Generation (RAG) module for two-person motion that retrieves motion exemplars using music features and LLM-based text decompositions of spatial relations, body movements, and rhythmic patterns. We use contrastive rectified flow objective to further sharpen alignment with conditioning signals and add synchronization loss to improve inter-person temporal coordination. Extensive evaluations across interactive, reactive, and multi-modal benchmarks demonstrate that DualFlow consistently improves motion quality, responsiveness, and semantic fidelity. DualFlow achieves state-of-the-art performance in two-person multi-modal motion generation, producing coherent, expressive, and rhythmically synchronized motion.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Static Images Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
<!-- Image 2 -->
      <div class="item">
        <div class="has-text-centered">
          <img src="static/images/Dualflow_model.png"
               alt="DualFlow Model"
               style="max-width: 70%; height: auto; margin: 0 auto;" />
        </div>
        <h2 class="subtitle is-6 has-text-centered">
          (a) Our framework takes text (CLIP-L/14), music, and motion sequences from an actor (A) and reactor (B) as inputs. Motion samples are retrieved using music features and LLM-decomposed text cues (spatial relationship, body movement, rhythm). These modality-specific latents are processed by cascaded Multi-Modal DualFlow Blocks that model interactive dynamics. Outputs are either both actors’ motions (interactive) or only the reactor’s motion (reactive) via a masking mechanism. (b) A DualFlow Block: in the interactive setting, both branches operate symmetrically with Motion Cross Attention coordinating joint motion; in the reactive setting, the actor branch is masked and the reactor branch employs a Causal Cross Attention module with Look-Ahead L, replacing Motion Cross Attention, to condition on the actor’s motion.
        </h2>
      </div>
  </div>
  </div>
</section>
<!-- End Static Images Section -->


<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/slFNUzFtQ68?rel=0&modestbranding=1&enablejsapi=1" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{gupta2025unifiedmultimodalinteractive,
      title={Unified Multi-Modal Interactive & Reactive 3D Motion Generation via Rectified Flow}, 
      author={Prerit Gupta and Shourya Verma and Ananth Grama and Aniket Bera},
      year={2025},
      eprint={2509.24099},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2509.24099}, 
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>

  <script>
    document.addEventListener("DOMContentLoaded", () => {
      const lazyIframes = document.querySelectorAll('iframe[data-src]');
      if ('IntersectionObserver' in window) {
        const io = new IntersectionObserver((entries) => {
          entries.forEach(({ isIntersecting, target }) => {
            if (!isIntersecting) return;
            target.src = target.dataset.src;
            io.unobserve(target);
          });
        }, { rootMargin: '200px' });
        
        lazyIframes.forEach(iframe => io.observe(iframe));
      } else {
        // fallback: load them all immediately
        lazyIframes.forEach(iframe => iframe.src = iframe.dataset.src);
      }
    });
  </script>
  
